"""
ETL Data Quality Pipeline
==========================
Generated by: IDE AI (GitHub Copilot in VS Code)
Input:         specs/data_quality_spec.json  (produced by Chat AI)
Purpose:       Reads the Chat AI-generated quality spec and applies all
               validation rules and transformations to raw sales data.

This is Step 2 of the AI-powered workflow:
  Chat AI (spec) --> IDE AI (this pipeline) --> CLI AI (run & summarize)
"""

import json
import re
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime


# ──────────────────────────────────────────────
# 1. LOAD THE CHAT-AI-GENERATED SPEC
# ──────────────────────────────────────────────
def load_spec(spec_path: str) -> dict:
    """Load the data quality specification JSON produced by the Chat AI."""
    with open(spec_path, "r", encoding="utf-8") as f:
        return json.load(f)


# ──────────────────────────────────────────────
# 2. QUALITY CHECK FUNCTIONS  (one per rule)
# ──────────────────────────────────────────────
def check_duplicates(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R001 – Remove exact duplicate rows."""
    cols = rule["columns"]
    dup_mask = df.duplicated(subset=cols, keep="first")
    issues = []
    if dup_mask.any():
        dup_indices = df[dup_mask].index.tolist()
        issues.append({
            "rule_id": rule["rule_id"],
            "rule_name": rule["name"],
            "rows_affected": len(dup_indices),
            "sample_rows": dup_indices[:5],
            "detail": f"Found {len(dup_indices)} duplicate row(s)"
        })
        df = df[~dup_mask].copy()
    return df, issues


def check_nulls(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R002 – Flag rows with null / empty values in critical columns."""
    issues = []
    flagged_indices = set()
    for col in rule["columns"]:
        if col in df.columns:
            null_mask = df[col].isna() | (df[col].astype(str).str.strip() == "")
            bad = df[null_mask].index.tolist()
            if bad:
                flagged_indices.update(bad)
                issues.append({
                    "rule_id": rule["rule_id"],
                    "rule_name": rule["name"],
                    "column": col,
                    "rows_affected": len(bad),
                    "sample_rows": bad[:5],
                    "detail": f"Column '{col}' has {len(bad)} null/empty value(s)"
                })
    # Mark flagged rows
    df["_null_flag"] = df.index.isin(flagged_indices)
    return df, issues


def validate_email(df: pd.DataFrame, rule: dict, schema: dict) -> tuple[pd.DataFrame, list]:
    """R003 – Validate email format."""
    issues = []
    col = rule["column"]
    pattern = schema[col]["pattern"]
    if col in df.columns:
        valid_mask = df[col].astype(str).apply(
            lambda x: bool(re.match(pattern, x)) if pd.notna(x) and x.strip() else False
        )
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            bad_idx = df[~valid_mask].index.tolist()
            issues.append({
                "rule_id": rule["rule_id"],
                "rule_name": rule["name"],
                "column": col,
                "rows_affected": int(invalid_count),
                "sample_rows": bad_idx[:5],
                "sample_values": df.loc[bad_idx[:5], col].tolist(),
                "detail": f"{invalid_count} invalid email(s) found"
            })
        df["_email_valid"] = valid_mask
    return df, issues


def validate_quantity(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R004 – Quantity must be in [1, 1000]."""
    issues = []
    col = rule["column"]
    min_v, max_v = rule["min_value"], rule["max_value"]
    # Convert to numeric, coercing errors
    df[col] = pd.to_numeric(df[col], errors="coerce")
    out_of_range = df[(df[col] < min_v) | (df[col] > max_v) | df[col].isna()]
    if len(out_of_range):
        bad_idx = out_of_range.index.tolist()
        issues.append({
            "rule_id": rule["rule_id"],
            "rule_name": rule["name"],
            "column": col,
            "rows_affected": len(bad_idx),
            "sample_rows": bad_idx[:5],
            "sample_values": df.loc[bad_idx[:5], col].tolist(),
            "detail": f"{len(bad_idx)} row(s) with quantity outside [{min_v}, {max_v}]"
        })
    df["_qty_valid"] = ~df.index.isin(out_of_range.index)
    return df, issues


def validate_price(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R005 – Unit price must be positive."""
    issues = []
    col = rule["column"]
    min_v = rule["min_value"]
    df[col] = pd.to_numeric(df[col], errors="coerce")
    bad_mask = (df[col] < min_v) | df[col].isna()
    if bad_mask.any():
        bad_idx = df[bad_mask].index.tolist()
        issues.append({
            "rule_id": rule["rule_id"],
            "rule_name": rule["name"],
            "column": col,
            "rows_affected": len(bad_idx),
            "sample_rows": bad_idx[:5],
            "sample_values": df.loc[bad_idx[:5], col].tolist(),
            "detail": f"{len(bad_idx)} row(s) with non-positive price"
        })
    df["_price_valid"] = ~bad_mask
    return df, issues


def standardize_dates(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R006/R007 – Parse multiple date formats into YYYY-MM-DD; flag unparseable."""
    issues = []
    col = rule["column"]
    parsed = pd.to_datetime(df[col], format="mixed", dayfirst=False, errors="coerce")
    invalid_mask = parsed.isna()
    if invalid_mask.any():
        bad_idx = df[invalid_mask].index.tolist()
        issues.append({
            "rule_id": rule["rule_id"],
            "rule_name": rule["name"],
            "column": col,
            "rows_affected": len(bad_idx),
            "sample_rows": bad_idx[:5],
            "sample_values": df.loc[bad_idx[:5], col].tolist(),
            "detail": f"{len(bad_idx)} unparseable / invalid date(s)"
        })
    df[col] = parsed.dt.strftime("%Y-%m-%d")
    df["_date_valid"] = ~invalid_mask
    return df, issues


def normalize_status(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R008 – Lowercase status values."""
    issues = []
    col = rule["column"]
    original = df[col].copy()
    df[col] = df[col].astype(str).str.strip().str.lower()
    changed = (original != df[col])
    if changed.any():
        issues.append({
            "rule_id": rule["rule_id"],
            "rule_name": rule["name"],
            "column": col,
            "rows_affected": int(changed.sum()),
            "detail": f"{int(changed.sum())} status value(s) normalized to lowercase"
        })
    return df, issues


def validate_payment(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R009 – Payment method must be in allowed list."""
    issues = []
    col = rule["column"]
    allowed = rule["allowed"]
    invalid_mask = ~df[col].isin(allowed)
    if invalid_mask.any():
        bad_idx = df[invalid_mask].index.tolist()
        issues.append({
            "rule_id": rule["rule_id"],
            "rule_name": rule["name"],
            "column": col,
            "rows_affected": len(bad_idx),
            "sample_rows": bad_idx[:5],
            "sample_values": df.loc[bad_idx[:5], col].tolist(),
            "detail": f"{len(bad_idx)} row(s) with invalid payment method"
        })
    df["_payment_valid"] = ~invalid_mask
    return df, issues


def compute_total(df: pd.DataFrame, rule: dict) -> tuple[pd.DataFrame, list]:
    """R010 – Add total_amount = quantity * unit_price."""
    df["total_amount"] = (
        pd.to_numeric(df["quantity"], errors="coerce") *
        pd.to_numeric(df["unit_price"], errors="coerce")
    ).round(2)
    return df, []


# ──────────────────────────────────────────────
# 3. PIPELINE ORCHESTRATOR
# ──────────────────────────────────────────────
def run_pipeline(spec: dict) -> dict:
    """Execute the full ETL pipeline driven by the Chat AI spec."""
    base_dir = Path(__file__).parent
    src = base_dir / spec["source_file"]
    out_clean = base_dir / spec["output_file"]
    out_report = base_dir / spec["report_file"]

    # ── Step 1: Load ──
    print(f"[1/14] Loading raw data from {src} ...")
    df = pd.read_csv(src)
    initial_rows = len(df)
    print(f"       Loaded {initial_rows} rows, {len(df.columns)} columns.\n")

    # ── Step 2: Strip whitespace ──
    print("[2/14] Stripping whitespace ...")
    str_cols = df.select_dtypes(include=["object", "string"]).columns
    df[str_cols] = df[str_cols].apply(lambda c: c.str.strip())

    # ── Step 3: Replace null-like strings ──
    print("[3/14] Replacing null-like strings with NaN ...")
    df.replace({"NULL": np.nan, "null": np.nan, "": np.nan, "None": np.nan}, inplace=True)

    all_issues = []
    rules = {r["rule_id"]: r for r in spec["quality_rules"]}
    schema = spec["schema"]

    # ── Step 4: Deduplication (R001) ──
    print("[4/14] Checking for duplicates (R001) ...")
    df, issues = check_duplicates(df, rules["R001"])
    all_issues.extend(issues)
    print(f"       Rows after dedup: {len(df)}\n")

    # ── Step 5: Null checks (R002) ──
    print("[5/14] Checking for nulls in critical columns (R002) ...")
    df, issues = check_nulls(df, rules["R002"])
    all_issues.extend(issues)

    # ── Step 6: Standardize dates (R006) ──
    print("[6/14] Standardizing date formats (R006/R007) ...")
    df, issues = standardize_dates(df, rules["R006"])
    all_issues.extend(issues)

    # ── Step 7: Normalize status (R008) ──
    print("[7/14] Normalizing status values (R008) ...")
    df, issues = normalize_status(df, rules["R008"])
    all_issues.extend(issues)

    # ── Step 8: Validate emails (R003) ──
    print("[8/14] Validating email format (R003) ...")
    df, issues = validate_email(df, rules["R003"], schema)
    all_issues.extend(issues)

    # ── Step 9: Validate quantity (R004) ──
    print("[9/14] Validating quantity range (R004) ...")
    df, issues = validate_quantity(df, rules["R004"])
    all_issues.extend(issues)

    # ── Step 10: Validate price (R005) ──
    print("[10/14] Validating unit price (R005) ...")
    df, issues = validate_price(df, rules["R005"])
    all_issues.extend(issues)

    # ── Step 11: Validate payment method (R009) ──
    print("[11/14] Validating payment method (R009) ...")
    df, issues = validate_payment(df, rules["R009"])
    all_issues.extend(issues)

    # ── Step 12: Compute total amount (R010) ──
    print("[12/14] Computing total_amount (R010) ...")
    df, issues = compute_total(df, rules["R010"])

    # ── Step 13: Separate clean vs flagged ──
    print("[13/14] Separating clean vs flagged rows ...")
    flag_cols = [c for c in df.columns if c.startswith("_")]
    validity_cols = [c for c in flag_cols if c != "_null_flag"]

    # A row is "clean" if all validity flags are True and null_flag is False
    clean_mask = pd.Series(True, index=df.index)
    for fc in flag_cols:
        if fc == "_null_flag":
            clean_mask &= ~df[fc]
        else:
            clean_mask &= df[fc]

    df_clean = df[clean_mask].drop(columns=flag_cols, errors="ignore").copy()
    df_flagged = df[~clean_mask].copy()

    print(f"       Clean rows:   {len(df_clean)}")
    print(f"       Flagged rows: {len(df_flagged)}\n")

    # ── Step 14: Export results ──
    print("[14/14] Exporting results ...")
    out_clean.parent.mkdir(parents=True, exist_ok=True)
    out_report.parent.mkdir(parents=True, exist_ok=True)

    df_clean.to_csv(out_clean, index=False)
    if len(df_flagged):
        flagged_path = base_dir / "data" / "flagged_rows.csv"
        df_flagged.to_csv(flagged_path, index=False)

    # Build quality report
    report = {
        "pipeline_name": spec["pipeline_name"],
        "run_timestamp": datetime.now().isoformat(),
        "spec_generated_by": spec["generated_by"],
        "summary": {
            "total_input_rows": initial_rows,
            "rows_after_dedup": len(df),
            "clean_rows": len(df_clean),
            "flagged_rows": len(df_flagged),
            "pass_rate_pct": round(len(df_clean) / len(df) * 100, 1) if len(df) else 0,
            "total_issues_found": len(all_issues)
        },
        "issues": all_issues,
        "output_files": {
            "cleaned_data": str(out_clean),
            "flagged_data": str(base_dir / "data" / "flagged_rows.csv"),
            "this_report": str(out_report)
        }
    }

    with open(out_report, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, default=str)

    print(f"       Cleaned data  → {out_clean}")
    print(f"       Quality report → {out_report}\n")

    # ── Final Summary ──
    print("=" * 55)
    print("  PIPELINE COMPLETE — QUALITY SUMMARY")
    print("=" * 55)
    print(f"  Input rows:        {initial_rows}")
    print(f"  After dedup:       {len(df)}")
    print(f"  Clean rows:        {len(df_clean)}")
    print(f"  Flagged rows:      {len(df_flagged)}")
    print(f"  Pass rate:         {report['summary']['pass_rate_pct']}%")
    print(f"  Issues detected:   {len(all_issues)}")
    print("=" * 55)

    return report


# ──────────────────────────────────────────────
# 4. ENTRY POINT
# ──────────────────────────────────────────────
if __name__ == "__main__":
    spec_path = Path(__file__).parent / "specs" / "data_quality_spec.json"
    spec = load_spec(str(spec_path))
    report = run_pipeline(spec)
